{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ma3bvPOsUy9Y",
        "outputId": "35a3f701-b915-4cbe-8513-09b0376f4479"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10\n",
            "['thirty-three', 'scotty', 'did', 'not', 'go', 'back', 'to', 'school', '.', 'his', 'parents', 'talked', 'seriously', 'and', 'lengthily', 'to', 'their', 'own', 'doctor', 'and', 'to', 'a', 'specialist', 'at', 'the', 'university', 'hospital', '--', 'mr.', 'mckinley', 'was', 'entitled', 'to', 'a', 'discount', 'for', 'members', 'of', 'his', 'family', '--', 'and', 'it', 'was', 'decided', 'it', 'would', 'be', 'best', 'for', 'him', 'to', 'take', 'the', 'remainder', 'of', 'the', 'term', 'off', ',', 'spend', 'a', 'lot', 'of', 'time', 'in', 'bed', 'and', ',', 'for', 'the', 'rest', ',', 'do', 'pretty', 'much', 'as', 'he', 'chose', '--', 'provided', ',', 'of', 'course', ',', 'he', 'chose', 'to', 'do', 'nothing', 'too', 'exciting', 'or', 'too', 'debilitating', '.', 'his', 'teacher', 'and', 'his', 'school', 'principal', 'were', 'conferred', 'with', 'and', 'everyone', 'agreed', 'that', ',', 'if', 'he', 'kept', 'up', 'with', 'a', 'certain', 'amount', 'of', 'work', 'at', 'home', ',', 'there', 'was', 'little', 'danger', 'of', 'his', 'losing', 'a', 'term', '.', 'scotty', 'accepted', 'the', 'decision', 'with', 'indifference', 'and', 'did', 'not', 'enter', 'the', 'arguments', '.', 'he', 'was', 'discharged', 'from', 'the', 'hospital', 'after', 'a', 'two-day', 'checkup', 'and', 'he', 'and', 'his', 'parents', 'had', 'what', 'mr.', 'mckinley', 'described', 'as', 'a', '``', 'celebration', 'lunch', '``', 'at', 'the', 'cafeteria', 'on', 'the', 'campus', '.', 'rachel', 'wore', 'a', 'smart', 'hat', 'and', ',', 'because', 'she', 'had', 'been', 'warned', 'recently', 'about', 'smoking', ',', 'puffed', 'at', 'her', 'cigarettes', 'through', 'a', 'long', 'ivory', 'holder', 'stained', 'with', 'lipstick', '.', 'scotty', \"'s\", 'father', 'sat', 'sprawled', 'in', 'his', 'chair', ',', 'angular', ',', 'alert', 'as', 'a', 'cricket', ',', 'looking', 'about', 'at', 'the', 'huge', 'stainless-steel', 'appointments', 'of', 'the', 'room', 'with', 'an', 'expression', 'of', 'proprietorship', '.', 'teachers', '--', 'men', 'who', 'wore', 'brown', 'suits', 'and', 'had', 'gray', 'hair', 'and', 'pleasant', 'smiles', '--', 'came', 'to', 'their', 'table', 'to', 'talk', 'shop', 'and', 'to', 'be', 'introduced', 'to', 'scotty', 'and', 'rachel', '.', 'rachel', 'was', 'polite', ',', 'scotty', 'indifferent', '.', 'they', 'ate', 'the', 'cafeteria', 'food', 'with', 'its', 'orange', 'sauces', 'and', 'scotty', 'gazed', 'without', 'interest', 'at', 'his', 'food', ',', 'the', 'teachers', ',', 'the', 'heroic']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Error loading punkt: <urlopen error [Errno 11001]\n",
            "[nltk_data]     getaddrinfo failed>\n",
            "[nltk_data] Error loading brown: <urlopen error [Errno 11001]\n",
            "[nltk_data]     getaddrinfo failed>\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.inference.tableau import Categories\n",
        "import re\n",
        "from nltk.corpus import brown\n",
        "#from nltk.corpus import stopwords\n",
        "from nltk import sent_tokenize\n",
        "nltk.download('punkt')\n",
        "nltk.download('brown')\n",
        "\n",
        "text = ' '.join(brown.words(categories = 'fiction')[:300])\n",
        "text = text.lower()\n",
        "text_sents = sent_tokenize(text)\n",
        "print(len(text_sents))\n",
        "text_words = word_tokenize(text)\n",
        "print(text_words)\n",
        "\n",
        "#cleaned_text = re.sub(r'\\d', '', text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "monZDPAoaEnE",
        "outputId": "8873f9b7-b16e-4838-fe5d-2571a2627cd7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to C:\\Users\\sara\n",
            "[nltk_data]     soft\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['thirty-three', 'scotty', 'go', 'back', 'school', '.', 'parents', 'talked', 'seriously', 'lengthily', 'doctor', 'specialist', 'university', 'hospital', '--', 'mr.', 'mckinley', 'entitled', 'discount', 'members', 'family', '--', 'decided', 'would', 'best', 'take', 'remainder', 'term', ',', 'spend', 'lot', 'time', 'bed', ',', 'rest', ',', 'pretty', 'much', 'chose', '--', 'provided', ',', 'course', ',', 'chose', 'nothing', 'exciting', 'debilitating', '.', 'teacher', 'school', 'principal', 'conferred', 'everyone', 'agreed', ',', 'kept', 'certain', 'amount', 'work', 'home', ',', 'little', 'danger', 'losing', 'term', '.', 'scotty', 'accepted', 'decision', 'indifference', 'enter', 'arguments', '.', 'discharged', 'hospital', 'two-day', 'checkup', 'parents', 'mr.', 'mckinley', 'described', '``', 'celebration', 'lunch', '``', 'cafeteria', 'campus', '.', 'rachel', 'wore', 'smart', 'hat', ',', 'warned', 'recently', 'smoking', ',', 'puffed', 'cigarettes']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "#to remove stopwords\n",
        "stop_words = nltk.corpus.stopwords.words('english')\n",
        "words_nostops = [w for w in text_words if w not in stop_words]\n",
        "print(words_nostops[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zr_oqIAYj5Ao",
        "outputId": "d8eda365-5bc2-46c0-9059-ff9a64f35384"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['thirtythree', 'scotty', 'go', 'back', 'school', 'parents', 'talked', 'seriously', 'lengthily', 'doctor', 'specialist', 'university', 'hospital', 'mr', 'mckinley', 'entitled', 'discount', 'members', 'family', 'decided', 'would', 'best', 'take', 'remainder', 'term', 'spend', 'lot', 'time', 'bed', 'rest', 'pretty', 'much', 'chose', 'provided', 'course', 'chose', 'nothing', 'exciting', 'debilitating', 'teacher', 'school', 'principal', 'conferred', 'everyone', 'agreed', 'kept', 'certain', 'amount', 'work', 'home', 'little', 'danger', 'losing', 'term', 'scotty', 'accepted', 'decision', 'indifference', 'enter', 'arguments', 'discharged', 'hospital', 'twoday', 'checkup', 'parents', 'mr', 'mckinley', 'described', 'celebration', 'lunch', 'cafeteria', 'campus', 'rachel', 'wore', 'smart', 'hat', 'warned', 'recently', 'smoking', 'puffed', 'cigarettes', 'long', 'ivory', 'holder', 'stained', 'lipstick', 'scotty', 'father', 'sat', 'sprawled', 'chair', 'angular', 'alert', 'cricket', 'looking', 'huge', 'stainlesssteel', 'appointments', 'room', 'expression', 'proprietorship', 'teachers', 'men', 'wore', 'brown', 'suits', 'gray', 'hair', 'pleasant', 'smiles', 'came', 'table', 'talk', 'shop', 'introduced', 'scotty', 'rachel', 'rachel', 'polite', 'scotty', 'indifferent', 'ate', 'cafeteria', 'food', 'orange', 'sauces', 'scotty', 'gazed', 'without', 'interest', 'food', 'teachers', 'heroic']\n"
          ]
        }
      ],
      "source": [
        "import string\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "text_stripped = [w.translate(table) for w in words_nostops]\n",
        "for s in text_stripped:\n",
        "  if s == '' or len(s)<2:\n",
        "    text_stripped.remove(s)\n",
        "print(text_stripped)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEIrcQQQvK6D",
        "outputId": "b85fe304-9427-49b8-a319-83a0422283f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original Word: thirtythree \t Stemmed Word: thirtythre\n",
            "Original Word: scotty \t Stemmed Word: scotti\n",
            "Original Word: go \t Stemmed Word: go\n",
            "Original Word: back \t Stemmed Word: back\n",
            "Original Word: school \t Stemmed Word: school\n",
            "Original Word: parents \t Stemmed Word: parent\n",
            "Original Word: talked \t Stemmed Word: talk\n",
            "Original Word: seriously \t Stemmed Word: serious\n",
            "Original Word: lengthily \t Stemmed Word: lengthili\n",
            "Original Word: doctor \t Stemmed Word: doctor\n",
            "Original Word: specialist \t Stemmed Word: specialist\n",
            "Original Word: university \t Stemmed Word: univers\n",
            "Original Word: hospital \t Stemmed Word: hospit\n",
            "Original Word: mr \t Stemmed Word: mr\n",
            "Original Word: mckinley \t Stemmed Word: mckinley\n",
            "Original Word: entitled \t Stemmed Word: entitl\n",
            "Original Word: discount \t Stemmed Word: discount\n",
            "Original Word: members \t Stemmed Word: member\n",
            "Original Word: family \t Stemmed Word: famili\n",
            "Original Word: decided \t Stemmed Word: decid\n",
            "Original Word: would \t Stemmed Word: would\n",
            "Original Word: best \t Stemmed Word: best\n",
            "Original Word: take \t Stemmed Word: take\n",
            "Original Word: remainder \t Stemmed Word: remaind\n",
            "Original Word: term \t Stemmed Word: term\n",
            "Original Word: spend \t Stemmed Word: spend\n",
            "Original Word: lot \t Stemmed Word: lot\n",
            "Original Word: time \t Stemmed Word: time\n",
            "Original Word: bed \t Stemmed Word: bed\n",
            "Original Word: rest \t Stemmed Word: rest\n",
            "Original Word: pretty \t Stemmed Word: pretti\n",
            "Original Word: much \t Stemmed Word: much\n",
            "Original Word: chose \t Stemmed Word: chose\n",
            "Original Word: provided \t Stemmed Word: provid\n",
            "Original Word: course \t Stemmed Word: cours\n",
            "Original Word: chose \t Stemmed Word: chose\n",
            "Original Word: nothing \t Stemmed Word: noth\n",
            "Original Word: exciting \t Stemmed Word: excit\n",
            "Original Word: debilitating \t Stemmed Word: debilit\n",
            "Original Word: teacher \t Stemmed Word: teacher\n",
            "Original Word: school \t Stemmed Word: school\n",
            "Original Word: principal \t Stemmed Word: princip\n",
            "Original Word: conferred \t Stemmed Word: confer\n",
            "Original Word: everyone \t Stemmed Word: everyon\n",
            "Original Word: agreed \t Stemmed Word: agre\n",
            "Original Word: kept \t Stemmed Word: kept\n",
            "Original Word: certain \t Stemmed Word: certain\n",
            "Original Word: amount \t Stemmed Word: amount\n",
            "Original Word: work \t Stemmed Word: work\n",
            "Original Word: home \t Stemmed Word: home\n",
            "Original Word: little \t Stemmed Word: littl\n",
            "Original Word: danger \t Stemmed Word: danger\n",
            "Original Word: losing \t Stemmed Word: lose\n",
            "Original Word: term \t Stemmed Word: term\n",
            "Original Word: scotty \t Stemmed Word: scotti\n",
            "Original Word: accepted \t Stemmed Word: accept\n",
            "Original Word: decision \t Stemmed Word: decis\n",
            "Original Word: indifference \t Stemmed Word: indiffer\n",
            "Original Word: enter \t Stemmed Word: enter\n",
            "Original Word: arguments \t Stemmed Word: argument\n",
            "Original Word: discharged \t Stemmed Word: discharg\n",
            "Original Word: hospital \t Stemmed Word: hospit\n",
            "Original Word: twoday \t Stemmed Word: twoday\n",
            "Original Word: checkup \t Stemmed Word: checkup\n",
            "Original Word: parents \t Stemmed Word: parent\n",
            "Original Word: mr \t Stemmed Word: mr\n",
            "Original Word: mckinley \t Stemmed Word: mckinley\n",
            "Original Word: described \t Stemmed Word: describ\n",
            "Original Word: celebration \t Stemmed Word: celebr\n",
            "Original Word: lunch \t Stemmed Word: lunch\n",
            "Original Word: cafeteria \t Stemmed Word: cafeteria\n",
            "Original Word: campus \t Stemmed Word: campu\n",
            "Original Word: rachel \t Stemmed Word: rachel\n",
            "Original Word: wore \t Stemmed Word: wore\n",
            "Original Word: smart \t Stemmed Word: smart\n",
            "Original Word: hat \t Stemmed Word: hat\n",
            "Original Word: warned \t Stemmed Word: warn\n",
            "Original Word: recently \t Stemmed Word: recent\n",
            "Original Word: smoking \t Stemmed Word: smoke\n",
            "Original Word: puffed \t Stemmed Word: puf\n",
            "Original Word: cigarettes \t Stemmed Word: cigarett\n",
            "Original Word: long \t Stemmed Word: long\n",
            "Original Word: ivory \t Stemmed Word: ivori\n",
            "Original Word: holder \t Stemmed Word: holder\n",
            "Original Word: stained \t Stemmed Word: stain\n",
            "Original Word: lipstick \t Stemmed Word: lipstick\n",
            "Original Word: scotty \t Stemmed Word: scotti\n",
            "Original Word: father \t Stemmed Word: father\n",
            "Original Word: sat \t Stemmed Word: sat\n",
            "Original Word: sprawled \t Stemmed Word: sprawl\n",
            "Original Word: chair \t Stemmed Word: chair\n",
            "Original Word: angular \t Stemmed Word: angular\n",
            "Original Word: alert \t Stemmed Word: alert\n",
            "Original Word: cricket \t Stemmed Word: cricket\n",
            "Original Word: looking \t Stemmed Word: look\n",
            "Original Word: huge \t Stemmed Word: huge\n",
            "Original Word: stainlesssteel \t Stemmed Word: stainlesssteel\n",
            "Original Word: appointments \t Stemmed Word: appoint\n",
            "Original Word: room \t Stemmed Word: room\n",
            "Original Word: expression \t Stemmed Word: express\n",
            "Original Word: proprietorship \t Stemmed Word: proprietorship\n",
            "Original Word: teachers \t Stemmed Word: teacher\n",
            "Original Word: men \t Stemmed Word: men\n",
            "Original Word: wore \t Stemmed Word: wore\n",
            "Original Word: brown \t Stemmed Word: brown\n",
            "Original Word: suits \t Stemmed Word: suit\n",
            "Original Word: gray \t Stemmed Word: gray\n",
            "Original Word: hair \t Stemmed Word: hair\n",
            "Original Word: pleasant \t Stemmed Word: pleasant\n",
            "Original Word: smiles \t Stemmed Word: smile\n",
            "Original Word: came \t Stemmed Word: came\n",
            "Original Word: table \t Stemmed Word: tabl\n",
            "Original Word: talk \t Stemmed Word: talk\n",
            "Original Word: shop \t Stemmed Word: shop\n",
            "Original Word: introduced \t Stemmed Word: introduc\n",
            "Original Word: scotty \t Stemmed Word: scotti\n",
            "Original Word: rachel \t Stemmed Word: rachel\n",
            "Original Word: rachel \t Stemmed Word: rachel\n",
            "Original Word: polite \t Stemmed Word: polit\n",
            "Original Word: scotty \t Stemmed Word: scotti\n",
            "Original Word: indifferent \t Stemmed Word: indiffer\n",
            "Original Word: ate \t Stemmed Word: ate\n",
            "Original Word: cafeteria \t Stemmed Word: cafeteria\n",
            "Original Word: food \t Stemmed Word: food\n",
            "Original Word: orange \t Stemmed Word: orang\n",
            "Original Word: sauces \t Stemmed Word: sauc\n",
            "Original Word: scotty \t Stemmed Word: scotti\n",
            "Original Word: gazed \t Stemmed Word: gaze\n",
            "Original Word: without \t Stemmed Word: without\n",
            "Original Word: interest \t Stemmed Word: interest\n",
            "Original Word: food \t Stemmed Word: food\n",
            "Original Word: teachers \t Stemmed Word: teacher\n",
            "Original Word: heroic \t Stemmed Word: heroic\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "porter = PorterStemmer()\n",
        "for word in text_stripped:\n",
        "  print(\"Original Word: {} \\t Stemmed Word: {}\".format(word,porter.stem(word)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fPK8SFFHZfvZ",
        "outputId": "3071be98-a9df-4be5-9a78-bb5317d7a65f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<ConditionalFreqDist with 153 conditions>\n",
            "The probability of the sentence \"thirtythree scotty did not go back to school \" is 0.010000000000000002\n",
            "The probability of the sentence \"his parents talked seriously and lengthily to their own doctor and to a specialist at the university hospital  mr mckinley was entitled to a discount for members of his family  and it was decided it would be best for him to take the remainder of the term off  spend a lot of time in bed and  for the rest  do pretty much as he chose  provided  of course  he chose to do nothing too exciting or too debilitating \" is 3.0269684423336354e-30\n",
            "The probability of the sentence \"his teacher and his school principal were conferred with and everyone agreed that  if he kept up with a certain amount of work at home  there was little danger of his losing a term \" is 5.023469650205761e-13\n",
            "The probability of the sentence \"scotty accepted the decision with indifference and did not enter the arguments \" is 5.66893424036281e-06\n",
            "The probability of the sentence \"he was discharged from the hospital after a twoday checkup and he and his parents had what mr mckinley described as a  celebration lunch  at the cafeteria on the campus \" is 8.998308318036211e-13\n",
            "The probability of the sentence \"rachel wore a smart hat and  because she had been warned recently about smoking  puffed at her cigarettes through a long ivory holder stained with lipstick \" is 5.144032921810698e-07\n",
            "The probability of the sentence \"scottys father sat sprawled in his chair  angular  alert as a cricket  looking about at the huge stainlesssteel appointments of the room with an expression of proprietorship \" is 2.768034297052154e-08\n",
            "The probability of the sentence \"teachers  men who wore brown suits and had gray hair and pleasant smiles  came to their table to talk shop and to be introduced to scotty and rachel \" is 3.2921810699588474e-11\n",
            "The probability of the sentence \"rachel was polite  scotty indifferent \" is 0.013333333333333334\n",
            "The probability of the sentence \"they ate the cafeteria food with its orange sauces and scotty gazed without interest at his food  the teachers  the heroic\" is 2.108978512039736e-09\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import string\n",
        "from nltk import bigrams\n",
        "from nltk.probability import FreqDist, ConditionalFreqDist\n",
        "\n",
        "# preprocessing sentences\n",
        "table = str.maketrans('', '', string.punctuation)\n",
        "sentences = [w.translate(table) for w in text_sents]\n",
        "\n",
        "\n",
        "# Tokenize the sentences\n",
        "tokenized_sentences = [nltk.word_tokenize(sentence) for sentence in sentences]\n",
        "\n",
        "# Flatten the list of words\n",
        "flat_words = [word for sublist in tokenized_sentences for word in sublist]\n",
        "#print(flat_words)\n",
        "# Calculate the frequency dist of words\n",
        "word_freq = FreqDist(flat_words)\n",
        "\n",
        "bi_grams = [list(bigrams(sentence)) for sentence in tokenized_sentences]\n",
        "#print(bi_grams)\n",
        "flat_bi_grams = [gram for sublist in bi_grams for gram in sublist]\n",
        "\n",
        "cfd_bi_grams = ConditionalFreqDist(flat_bi_grams)\n",
        "print(cfd_bi_grams)\n",
        "for sentence in sentences:\n",
        "    tokenized_sentence = nltk.word_tokenize(sentence)\n",
        "    sentence_bigrams = list(bigrams(tokenized_sentence))\n",
        "    sentence_probability = 1\n",
        "    for bigram in sentence_bigrams:\n",
        "        if bigram[0] in word_freq:\n",
        "            # MLE P(word|previous_word) = Count(previous_word, word) / Count(previous_word)\n",
        "            sentence_probability *= cfd_bi_grams[bigram[0]][bigram[1]] / word_freq[bigram[0]]\n",
        "        else:\n",
        "            sentence_probability = 0\n",
        "            break\n",
        "    print(f\"The probability of the sentence \\\"{sentence}\\\" is {sentence_probability}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Sc9MLXBbNYG"
      },
      "source": [
        "# ponus task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UP_7unO8bXg3",
        "outputId": "de2de2e0-a475-4d0e-85ae-910289265783"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Emails found:\n",
            "john.doe@email.com\n",
            "jane@example.com\n",
            "Phone numbers found:\n",
            "123-456-7890\n",
            "jj\n",
            "(987) 654-3210\n",
            "jj\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "# Sample document\n",
        "document = \"\"\"\n",
        "Hello, my email is john.doe@email.com, and my phone number is +123-456-7890.\n",
        "You can also reach me at jane@example.com or (987) 654-3210.\n",
        "\"\"\"\n",
        "\n",
        "# Regular expressions for emails and phone numbers\n",
        "email_pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b'\n",
        "phone_pattern = r'\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4}'\n",
        "\n",
        "# Find emails\n",
        "emails = re.findall(email_pattern, document)\n",
        "\n",
        "# Find phone numbers\n",
        "phone_numbers = re.findall(phone_pattern, document)\n",
        "\n",
        "# Print the results\n",
        "print(\"Emails found:\")\n",
        "for email in emails:\n",
        "  print(email)\n",
        "\n",
        "print(\"Phone numbers found:\")\n",
        "for phone in phone_numbers:\n",
        "  print(phone)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
